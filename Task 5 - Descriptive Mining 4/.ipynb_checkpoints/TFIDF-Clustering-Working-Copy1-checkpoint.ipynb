{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find which cluster means what...\n",
    "# Plot the density heatmap of largest 5 clusters' Locations\n",
    "# Implement 3-shingles...\n",
    "# Do the same with Negative Reviews (Plot etc)\n",
    "# find intersecting reviews between positive and negative clusters. Compare scores\n",
    "# Plot density heatmap of intersecting reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sps\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import langid\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change this to whichever CSV YOU WANT. But In the 2nd week, I constructed a csv file that also has the 'City' name column.\n",
    "# I'm using this csv for simplicity. You can construct the same by following my code in 2nd week.\n",
    "hotelReviews = pd.read_csv('C:/Users/deniz/Documents/HotelReviewsCountryCity.csv')\n",
    "cities = ['Paris', 'Amsterdam', 'London', 'Milan', 'Barcelona', 'Vienna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For this, I've used Only UK reviews on Barcelona. Change it as you wish.\n",
    "# Don't use more than 30000 texts with the function tho. Because it gets impossible to construct the cos similarity matrix.\n",
    "\n",
    "#allPosRevs = hotelReviews.loc[(hotelReviews['City'] == 'Barcelona') & (hotelReviews['Reviewer_Nationality'] == ' United Kingdom ') & (hotelReviews['Positive_Review'] != 'No Positive')]['Positive_Review']\n",
    "allPosRevs = hotelReviews.loc[(hotelReviews['City'] == 'Barcelona') & (hotelReviews['Reviewer_Nationality'] == ' United Kingdom ')]['Positive_Review']\n",
    "\n",
    "#allPosRevs = hotelReviews['Positive_Review']\n",
    "#allPosWords = hotelReviews.loc[hotelReviews['Positive_Review'] != 'No Positive']['Positive_Review']\n",
    "#allReviews = hotelReviews.loc[(hotelReviews['Positive_Review'] != 'No Positive') & (hotelReviews['Negative_Review'] != 'No Negative')][['Positive_Review', 'Negative_Review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myTokenizer(word, stop_words = stopwords.words('English'), wordLengthThreshold = 3):\n",
    "    tks = word.split()\n",
    "    s = []\n",
    "    for tok in tks:\n",
    "        if tok not in stop_words and len(tok) > wordLengthThreshold:\n",
    "            s.append(tok)\n",
    "    tokens = list(s)\n",
    "    for i in range(len(s)-1):\n",
    "        tokens.append(s[i] + ' ' + s[i+1])\n",
    "    for i in range(len(s)-2):\n",
    "        tokens.append(s[i] + ' ' + s[i+1] + ' ' + s[i+2])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ['hello world and deniz', 'deniz world word', 'hello deniz world']\n",
    "vocab = ['hello','world','deniz', 'world deniz', 'hello world', 'deniz world', ]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('English'), tokenizer=myTokenizer)\n",
    "featureMatrix = vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x12 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 18 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.28561676,  0.        ,  0.        ,  0.36778358,  0.        ,\n",
       "          0.        ,  0.48359121,  0.48359121,  0.        ,  0.28561676,\n",
       "          0.48359121,  0.        ],\n",
       "        [ 0.28561676,  0.36778358,  0.48359121,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.48359121,  0.28561676,\n",
       "          0.        ,  0.48359121],\n",
       "        [ 0.30083189,  0.38737583,  0.        ,  0.38737583,  0.50935267,\n",
       "          0.50935267,  0.        ,  0.        ,  0.        ,  0.30083189,\n",
       "          0.        ,  0.        ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureMatrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureMatrix = TfidfVectorizer(stop_words=stopwords.words('English'), tokenizer=myTokenizer).fit_transform(allPosRevs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20803x8357 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 226520 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.28561676  0.          0.          0.36778358  0.          0.\n",
      "  0.48359121  0.48359121  0.          0.28561676  0.48359121  0.        ]\n",
      "[ 0.28561676  0.36778358  0.48359121  0.          0.          0.          0.\n",
      "  0.          0.48359121  0.28561676  0.          0.48359121]\n",
      "[ 0.30083189  0.38737583  0.          0.38737583  0.50935267  0.50935267\n",
      "  0.          0.          0.          0.30083189  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "for p in np.array(featureMatrix.todense()):\n",
    "    print p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reviewWordList(reviews, wordLengthThreshold = 3, stopWordList = stopwords.words('English'), displayProgress = False):\n",
    "    allWords = []\n",
    "    i = 1\n",
    "    for posRev in reviews:\n",
    "        if i % ((len(reviews))/10) == 0 and displayProgress:\n",
    "            print '    ', 100 * (1.0 * i / len(reviews)), '% finished'\n",
    "        i+=1\n",
    "        allWords.append([f.split() for f in re.findall('\\d+|\\D+',posRev.lower())])\n",
    "\n",
    "    allWords = list(itertools.chain.from_iterable(list(itertools.chain.from_iterable(allWords))))\n",
    "    allWords = list(set(allWords))\n",
    "    revWordLst = []\n",
    "    for w in allWords:\n",
    "        if len(w) > wordLengthThreshold and w not in stopWordList:\n",
    "            revWordLst.append(w)\n",
    "    return revWordLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSimilarIndices(cosSim, simThreshold = .8, displayProgress = False):\n",
    "    simRevs = []\n",
    "\n",
    "    i = 1\n",
    "    for index in range(cosSim.shape[0]):\n",
    "        if i % (cosSim.shape[0]/10) == 0 and displayProgress:\n",
    "            print '    ', 100 * (1.0 * i / cosSim.shape[0]), '% finished'\n",
    "        i+=1\n",
    "        i_0 = np.array(cosSim[index].todense())[0]\n",
    "        sims = []\n",
    "        for where in np.argwhere(i_0 > simThreshold):\n",
    "            if where[0] != index:\n",
    "                #print where[0], lAPR[where[0]]\n",
    "                sims.append(where[0])\n",
    "        simRevs.append([index, sims])\n",
    "    return simRevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getConnectionMatrix(simRevs, displayProgress = False):\n",
    "    connectionMatrix = sps.lil_matrix((len(simRevs),len(simRevs))).astype(np.bool)\n",
    "    i = 1\n",
    "    for s in simRevs:\n",
    "        if i % ((len(simRevs))/10) == 0 and displayProgress:\n",
    "            print '    ', 100 * (1.0 * i / len(simRevs)), '% finished'\n",
    "        i+=1\n",
    "        ind = s[0]\n",
    "        for j in s[1]:\n",
    "            connectionMatrix[ind,j] = 1\n",
    "    return connectionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSimilarTexts(reviews, simThreshold = .8, displayProgress = False, wordLengthThreshold = 3, stopWordList = stopwords.words('English'), getTFIDFMatrix = False):\n",
    "    if displayProgress:\n",
    "        print \"Extracting Important Texts\"\n",
    "    rwl = reviewWordList(reviews, wordLengthThreshold = 3, stopWordList = stopwords.words('English'), displayProgress=displayProgress)\n",
    "    \n",
    "    if displayProgress:\n",
    "        print \"Constructing TF-IDF Vectorizer\"\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords.words('English'), vocabulary=rwl)\n",
    "    \n",
    "    if displayProgress:\n",
    "        print \"Constructing Feature Matrix\"\n",
    "    featureMatrix = vectorizer.fit_transform(reviews)\n",
    "    \n",
    "    if displayProgress:\n",
    "        print \"Constructing Cosine Similarity Matrix\"\n",
    "    cosSim = cosine_similarity(featureMatrix, Y=None, dense_output=False)\n",
    "    \n",
    "    if displayProgress:\n",
    "        print \"Extracting Similar Reviews\"\n",
    "    simRevs = getSimilarIndices(cosSim, simThreshold=simThreshold, displayProgress=displayProgress)\n",
    "    \n",
    "    if getTFIDFMatrix:\n",
    "        return simRevs, featureMatrix\n",
    "    else:\n",
    "        return simRevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getConnectivityGraph(reviews, simThreshold = .8, displayProgress = False, wordLengthThreshold = 3, stopWordList = stopwords.words('English'), getTFIDFMatrix = False):\n",
    "    if displayProgress:\n",
    "        print \"Extracting Important Texts\"\n",
    "    rwl = reviewWordList(reviews, wordLengthThreshold = 3, stopWordList = stopwords.words('English'), displayProgress=displayProgress)\n",
    "    \n",
    "    if displayProgress:\n",
    "        print \"Constructing Feature Matrix\"\n",
    "    featureMatrix = TfidfVectorizer(stop_words=stopwords.words('English'), vocabulary=rwl).fit_transform(reviews)\n",
    "    \n",
    "    if displayProgress:\n",
    "        print \"Constructing Cosine Similarity Matrix\"\n",
    "    cosSim = cosine_similarity(featureMatrix, Y=None, dense_output=False)\n",
    "    \n",
    "    if displayProgress:\n",
    "        print \"Extracting Similar Reviews\"\n",
    "    simRevs = getSimilarIndices(cosSim, simThreshold=simThreshold, displayProgress=displayProgress)\n",
    "    \n",
    "    if displayProgress:\n",
    "        print \"Constructing Connectivity Graph\"\n",
    "        connectionMatrix = getConnectionMatrix(simRevs, displayProgress=displayProgress)\n",
    "        \n",
    "    return connectionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printGraphImage(graph, fname = 'Image.bmp', sort = False):\n",
    "    if sort:\n",
    "        elems = np.array(graph.sum(axis=0))[0]\n",
    "        sortedIndices = np.argsort(elems)[::-1]\n",
    "        sortedConG = graph.tocsc()[:,sortedIndices]\n",
    "        sottedConG = sortedConG.tocsr()[sortedIndices,:]\n",
    "        sortedConG = sortedConG.tocsc()[:,sortedIndices]\n",
    "        sortedConG = sortedConG.tolil()\n",
    "        \n",
    "    else:\n",
    "        sortedConG = graph\n",
    "\n",
    "    dcImg = Image.new('1', sortedConG.shape, color=1)\n",
    "    pixels = dcImg.load()\n",
    "\n",
    "    nzs = sortedConG.nonzero()\n",
    "\n",
    "    for i in range(len(nzs[0])):\n",
    "        pixels[nzs[0][i], nzs[1][i]] = 0\n",
    "        \n",
    "    dcImg.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractClusters(graphMat, displayProgress = False):\n",
    "    conM = graphMat.copy()\n",
    "    clusters = []\n",
    "    iteration = 0\n",
    "    conSum = conM.sum()\n",
    "    while conSum > 0:\n",
    "        if displayProgress:\n",
    "            print \"iteration: \", iteration, \" conM.sum():\", conSum\n",
    "\n",
    "        index = np.argmax(np.array(conM.sum(axis=0))[0])\n",
    "        cluster = np.sort(np.insert(conM[index,:].nonzero()[1], 0, index))\n",
    "        clusters.append(cluster)\n",
    "        conM[cluster,:] = 0\n",
    "        conM[:,cluster] = 0\n",
    "        iteration += 1\n",
    "        conSum = conM.sum()\n",
    "        \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractClustersSingleDegree(graphMat, minClusterSize = -1,  displayProgress = False):\n",
    "    conM = graphMat.copy()\n",
    "    clusters = []\n",
    "    iteration = 0\n",
    "    conSum = conM.sum()\n",
    "    lastClusterSize = np.inf\n",
    "    \n",
    "    while conSum > 0 and lastClusterSize > minClusterSize:\n",
    "        if displayProgress:\n",
    "            print \"iteration: \", iteration, \" conM.sum():\", conSum\n",
    "\n",
    "        maxIndex = np.argmax(np.array(conM.sum(axis=0))[0])\n",
    "        clusterIndices = np.sort(np.insert(conM[maxIndex,:].nonzero()[1], 0, maxIndex))\n",
    "        \n",
    "        cluster = []\n",
    "        \n",
    "        for index in clusterIndices:\n",
    "            cluster.append(np.sort(np.insert(conM[index,:].nonzero()[1], 0, index)))\n",
    "            \n",
    "        cluster = np.array(list(set([item for sublist in cluster for item in sublist])))\n",
    "        clusters.append(cluster)\n",
    "        conM[cluster,:] = 0\n",
    "        conM[:,cluster] = 0\n",
    "        \n",
    "        iteration += 1\n",
    "        conSum = conM.sum()\n",
    "        lastClusterSize = cluster.shape[0]\n",
    "        \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractClustersExtensive(graphMat, minClusterSize = -1,  displayProgress = False):\n",
    "    conM = graphMat.copy()\n",
    "    clusters = []\n",
    "    iteration = 0\n",
    "    conSum = conM.sum()\n",
    "    lastClusterSize = np.inf\n",
    "    \n",
    "    while conSum > 0 and lastClusterSize > minClusterSize:\n",
    "        if displayProgress:\n",
    "            print \"iteration: \", iteration, \" conM.sum():\", conSum\n",
    "\n",
    "        connectionSums = np.array(conM.sum(axis=0))[0]\n",
    "        maxCon = np.max(connectionSums)\n",
    "        indices = np.where(connectionSums == maxCon)\n",
    "        \n",
    "        cluster = []\n",
    "        \n",
    "        for index in indices:\n",
    "            cluster.append(np.sort(np.insert(conM[index,:].nonzero()[1], 0, index)))\n",
    "            \n",
    "        cluster = np.array(list(set([item for sublist in cluster for item in sublist])))\n",
    "        clusters.append(cluster)\n",
    "        conM[cluster,:] = 0\n",
    "        conM[:,cluster] = 0\n",
    "        \n",
    "        iteration += 1\n",
    "        conSum = conM.sum()\n",
    "        lastClusterSize = cluster.shape[0]\n",
    "        \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Important Texts\n",
      "     9.9985579003 % finished\n",
      "     19.9971158006 % finished\n",
      "     29.9956737009 % finished\n",
      "     39.9942316012 % finished\n",
      "     49.9927895015 % finished\n",
      "     59.9913474018 % finished\n",
      "     69.9899053021 % finished\n",
      "     79.9884632024 % finished\n",
      "     89.9870211027 % finished\n",
      "     99.985579003 % finished\n",
      "Constructing Feature Matrix\n",
      "Constructing Cosine Similarity Matrix\n",
      "Extracting Similar Reviews\n",
      "     9.9985579003 % finished\n",
      "     19.9971158006 % finished\n",
      "     29.9956737009 % finished\n",
      "     39.9942316012 % finished\n",
      "     49.9927895015 % finished\n",
      "     59.9913474018 % finished\n",
      "     69.9899053021 % finished\n",
      "     79.9884632024 % finished\n",
      "     89.9870211027 % finished\n",
      "     99.985579003 % finished\n",
      "Constructing Connectivity Graph\n",
      "     9.9985579003 % finished\n",
      "     19.9971158006 % finished\n",
      "     29.9956737009 % finished\n",
      "     39.9942316012 % finished\n",
      "     49.9927895015 % finished\n",
      "     59.9913474018 % finished\n",
      "     69.9899053021 % finished\n",
      "     79.9884632024 % finished\n",
      "     89.9870211027 % finished\n",
      "     99.985579003 % finished\n"
     ]
    }
   ],
   "source": [
    "connectivityGraph = getConnectivityGraph(allPosRevs, simThreshold = .8, displayProgress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0  conM.sum(): 1154692\n",
      "iteration:  1  conM.sum(): 222502\n",
      "iteration:  2  conM.sum(): 115246\n",
      "iteration:  3  conM.sum(): 96178\n",
      "iteration:  4  conM.sum(): 75238\n",
      "iteration:  5  conM.sum(): 60552\n",
      "iteration:  6  conM.sum(): 48630\n",
      "iteration:  7  conM.sum(): 42290\n",
      "iteration:  8  conM.sum(): 38612\n",
      "iteration:  9  conM.sum(): 36062\n",
      "iteration:  10  conM.sum(): 34012\n",
      "iteration:  11  conM.sum(): 32152\n",
      "iteration:  12  conM.sum(): 30366\n",
      "iteration:  13  conM.sum(): 28884\n",
      "iteration:  14  conM.sum(): 27582\n",
      "iteration:  15  conM.sum(): 26172\n",
      "iteration:  16  conM.sum(): 25196\n",
      "iteration:  17  conM.sum(): 24348\n",
      "iteration:  18  conM.sum(): 23390\n",
      "iteration:  19  conM.sum(): 22602\n",
      "iteration:  20  conM.sum(): 21764\n",
      "iteration:  21  conM.sum(): 20920\n",
      "iteration:  22  conM.sum(): 20050\n",
      "iteration:  23  conM.sum(): 19246\n",
      "iteration:  24  conM.sum(): 18432\n",
      "iteration:  25  conM.sum(): 17754\n",
      "iteration:  26  conM.sum(): 17104\n",
      "iteration:  27  conM.sum(): 16588\n",
      "iteration:  28  conM.sum(): 15934\n",
      "iteration:  29  conM.sum(): 15388\n",
      "iteration:  30  conM.sum(): 14824\n",
      "iteration:  31  conM.sum(): 14154\n",
      "iteration:  32  conM.sum(): 13614\n",
      "iteration:  33  conM.sum(): 13104\n",
      "iteration:  34  conM.sum(): 12628\n",
      "iteration:  35  conM.sum(): 12290\n",
      "iteration:  36  conM.sum(): 11964\n",
      "iteration:  37  conM.sum(): 11458\n",
      "iteration:  38  conM.sum(): 11038\n",
      "iteration:  39  conM.sum(): 10756\n",
      "iteration:  40  conM.sum(): 10402\n",
      "iteration:  41  conM.sum(): 10016\n",
      "iteration:  42  conM.sum(): 9666\n",
      "iteration:  43  conM.sum(): 9290\n",
      "iteration:  44  conM.sum(): 9052\n",
      "iteration:  45  conM.sum(): 8788\n",
      "iteration:  46  conM.sum(): 8542\n",
      "iteration:  47  conM.sum(): 8336\n",
      "iteration:  48  conM.sum(): 8100\n",
      "iteration:  49  conM.sum(): 7854\n",
      "iteration:  50  conM.sum(): 7654\n",
      "iteration:  51  conM.sum(): 7486\n",
      "iteration:  52  conM.sum(): 7270\n",
      "iteration:  53  conM.sum(): 7060\n",
      "iteration:  54  conM.sum(): 6924\n",
      "iteration:  55  conM.sum(): 6770\n",
      "iteration:  56  conM.sum(): 6634\n",
      "iteration:  57  conM.sum(): 6516\n",
      "iteration:  58  conM.sum(): 6320\n",
      "iteration:  59  conM.sum(): 6168\n",
      "iteration:  60  conM.sum(): 6044\n",
      "iteration:  61  conM.sum(): 5938\n",
      "iteration:  62  conM.sum(): 5836\n",
      "iteration:  63  conM.sum(): 5774\n",
      "iteration:  64  conM.sum(): 5684\n",
      "iteration:  65  conM.sum(): 5568\n",
      "iteration:  66  conM.sum(): 5490\n",
      "iteration:  67  conM.sum(): 5412\n",
      "iteration:  68  conM.sum(): 5312\n",
      "iteration:  69  conM.sum(): 5220\n",
      "iteration:  70  conM.sum(): 5138\n",
      "iteration:  71  conM.sum(): 5048\n",
      "iteration:  72  conM.sum(): 4982\n",
      "iteration:  73  conM.sum(): 4850\n",
      "iteration:  74  conM.sum(): 4778\n",
      "iteration:  75  conM.sum(): 4696\n",
      "iteration:  76  conM.sum(): 4646\n",
      "iteration:  77  conM.sum(): 4556\n",
      "iteration:  78  conM.sum(): 4496\n",
      "iteration:  79  conM.sum(): 4438\n",
      "iteration:  80  conM.sum(): 4362\n",
      "iteration:  81  conM.sum(): 4256\n",
      "iteration:  82  conM.sum(): 4182\n",
      "iteration:  83  conM.sum(): 4112\n",
      "iteration:  84  conM.sum(): 4048\n",
      "iteration:  85  conM.sum(): 3986\n",
      "iteration:  86  conM.sum(): 3916\n",
      "iteration:  87  conM.sum(): 3878\n",
      "iteration:  88  conM.sum(): 3822\n",
      "iteration:  89  conM.sum(): 3758\n",
      "iteration:  90  conM.sum(): 3728\n",
      "iteration:  91  conM.sum(): 3674\n",
      "iteration:  92  conM.sum(): 3636\n",
      "iteration:  93  conM.sum(): 3596\n",
      "iteration:  94  conM.sum(): 3544\n",
      "iteration:  95  conM.sum(): 3488\n",
      "iteration:  96  conM.sum(): 3426\n",
      "iteration:  97  conM.sum(): 3372\n",
      "iteration:  98  conM.sum(): 3336\n",
      "iteration:  99  conM.sum(): 3312\n",
      "iteration:  100  conM.sum(): 3284\n",
      "iteration:  101  conM.sum(): 3240\n",
      "iteration:  102  conM.sum(): 3184\n",
      "iteration:  103  conM.sum(): 3142\n",
      "iteration:  104  conM.sum(): 3106\n",
      "iteration:  105  conM.sum(): 3060\n",
      "iteration:  106  conM.sum(): 3020\n",
      "iteration:  107  conM.sum(): 2984\n",
      "iteration:  108  conM.sum(): 2946\n",
      "iteration:  109  conM.sum(): 2914\n",
      "iteration:  110  conM.sum(): 2872\n",
      "iteration:  111  conM.sum(): 2834\n",
      "iteration:  112  conM.sum(): 2806\n",
      "iteration:  113  conM.sum(): 2774\n",
      "iteration:  114  conM.sum(): 2742\n",
      "iteration:  115  conM.sum(): 2720\n",
      "iteration:  116  conM.sum(): 2680\n",
      "iteration:  117  conM.sum(): 2656\n",
      "iteration:  118  conM.sum(): 2620\n",
      "iteration:  119  conM.sum(): 2592\n",
      "iteration:  120  conM.sum(): 2558\n",
      "iteration:  121  conM.sum(): 2528\n",
      "iteration:  122  conM.sum(): 2500\n",
      "iteration:  123  conM.sum(): 2456\n",
      "iteration:  124  conM.sum(): 2434\n",
      "iteration:  125  conM.sum(): 2404\n",
      "iteration:  126  conM.sum(): 2374\n",
      "iteration:  127  conM.sum(): 2344\n",
      "iteration:  128  conM.sum(): 2320\n",
      "iteration:  129  conM.sum(): 2296\n",
      "iteration:  130  conM.sum(): 2274\n",
      "iteration:  131  conM.sum(): 2246\n",
      "iteration:  132  conM.sum(): 2218\n",
      "iteration:  133  conM.sum(): 2192\n",
      "iteration:  134  conM.sum(): 2166\n",
      "iteration:  135  conM.sum(): 2136\n",
      "iteration:  136  conM.sum(): 2108\n",
      "iteration:  137  conM.sum(): 2092\n",
      "iteration:  138  conM.sum(): 2064\n",
      "iteration:  139  conM.sum(): 2040\n",
      "iteration:  140  conM.sum(): 2002\n",
      "iteration:  141  conM.sum(): 1986\n",
      "iteration:  142  conM.sum(): 1968\n",
      "iteration:  143  conM.sum(): 1944\n",
      "iteration:  144  conM.sum(): 1922\n",
      "iteration:  145  conM.sum(): 1902\n",
      "iteration:  146  conM.sum(): 1878\n",
      "iteration:  147  conM.sum(): 1852\n"
     ]
    }
   ],
   "source": [
    "clusters = extractClustersSingleDegree(connectivityGraph, minClusterSize = 5, displayProgress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusterSizes = np.argsort([c.shape[0] for c in clusters])[::-1]\n",
    "sortedClusters = [clusters[cls] for cls in clusterSizes]\n",
    "sortIndex = np.array([item for sublist in sortedClusters for item in sublist])\n",
    "pltP = connectivityGraph[sortIndex,:]\n",
    "pltP = pltP[:,sortIndex]\n",
    "printGraphImage(pltP, 'clustered_single_deg_08.bmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1 = [i for i in range(connectivityGraph.shape[0])]\n",
    "l3 = np.array(list(set(l1) - set(sortIndex)))\n",
    "displayIndices = np.hstack([sortIndex,l3])\n",
    "pltP = connectivityGraph[displayIndices,:]\n",
    "pltP = pltP[:,displayIndices]\n",
    "printGraphImage(pltP, 'clustered_single_deg_allSamples_05.bmp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

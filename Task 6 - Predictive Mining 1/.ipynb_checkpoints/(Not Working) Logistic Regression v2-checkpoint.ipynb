{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize, MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishReviews = pd.read_csv('UKReviewsWordCounted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordSplit(rev, stop_words = stopwords.words('English')):\n",
    "    s = [item for sublist in [rev.lower().split() for f in re.findall('\\d+|\\D+',rev.lower())] for item in sublist]\n",
    "    s = [x for x in s if x not in stop_words]\n",
    "    return s\n",
    "\n",
    "def bagOfWordsShingles(review, k=3, stop_words = stopwords.words('English')):\n",
    "    s = wordSplit(review, stop_words)\n",
    "    if len(s) < k:\n",
    "        tokens = ['']\n",
    "        for s_ in s:\n",
    "            tokens[0] += s_ + ' '\n",
    "        tokens[0] = tokens[0][:-1]\n",
    "        return tokens\n",
    "        \n",
    "    tokens = []\n",
    "    for i in range(len(s) - k + 1):\n",
    "        bag = ''\n",
    "        for j in range(i, i + k):\n",
    "            bag += s[j]+' '\n",
    "        tokens.append(bag[:-1])\n",
    "    return tokens\n",
    "\n",
    "def bagOfWordsShingles2(review, k=3, stop_words = stopwords.words('English')):\n",
    "    s = wordSplit(review, stop_words)\n",
    "    tokens = []\n",
    "    for K in range(0,k):\n",
    "        for i in range(len(s) - (k-K) + 1):\n",
    "            bag = ''\n",
    "            for j in range(i, i + k-K):\n",
    "                bag += s[j]+' '\n",
    "            tokens.append(bag[:-1])\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateData(inputReviews, testSize = .33, wordThreshold = 3):\n",
    "    thrPosEngRevs = inputReviews.loc[inputReviews['Positive_Processed_Word_Count'] > wordThreshold]['Positive_Review']\n",
    "    thrNegEngRevs = inputReviews.loc[inputReviews['Negative_Processed_Word_Count'] > wordThreshold]['Negative_Review']\n",
    "\n",
    "    allReviews = pd.concat([thrPosEngRevs, thrNegEngRevs])\n",
    "    labels = np.hstack([np.ones(thrPosEngRevs.shape[0]), np.zeros(thrNegEngRevs.shape[0])])\n",
    "    train, test, train_labels, test_labels = train_test_split(range(len(labels)),labels,test_size=testSize,random_state=42)\n",
    "    \n",
    "    return allReviews.iloc[train], allReviews.iloc[test], train_labels, test_labels\n",
    "\n",
    "def extractFeatures(trData, teData, featureType=1):\n",
    "    if featureType == 1:\n",
    "        tfidf = TfidfVectorizer(stop_words=stopwords.words('English'),tokenizer=bagOfWordsShingles, lowercase=True)\n",
    "    elif featureType == 2:\n",
    "        tfidf = TfidfVectorizer(stop_words=stopwords.words('English'),tokenizer=bagOfWordsShingles2, lowercase=True)\n",
    "    else:\n",
    "        print \"Please select a feature extractor type\"\n",
    "        return -1,-1\n",
    "    print \"        Extracting Training Features\"\n",
    "    trFeatures = tfidf.fit_transform(trData)\n",
    "    \n",
    "    print \"        Extracting Testing Features\"\n",
    "    if featureType == 1:\n",
    "        teFeatures = TfidfVectorizer(stop_words=stopwords.words('English'),tokenizer=bagOfWordsShingles, lowercase=True, vocabulary = tfidf.vocabulary_).fit_transform(teData)\n",
    "    elif featureType == 2:\n",
    "        teFeatures = TfidfVectorizer(stop_words=stopwords.words('English'),tokenizer=bagOfWordsShingles2, lowercase=True, vocabulary = tfidf.vocabulary_).fit_transform(teData)\n",
    "    else:\n",
    "        print \"Please select a feature extractor type\"\n",
    "        return -1,-1\n",
    "\n",
    "    return trFeatures, teFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_logistic_regression(englishReviews, wordThreshold, featureType):\n",
    "    print \"wordThreshold = \", wordThreshold, \"featureType = \", featureType\n",
    "    trD, teD, trL, teL = separateData(englishReviews, wordThreshold)\n",
    "    trFeatures, teFeatures = extractFeatures(trD, teD, featureType)\n",
    "    print \"    Fitting the regressor\"\n",
    "    lreg = LogisticRegression(tol=0.001)\n",
    "    lreg.fit(trFeatures,trL)\n",
    "    print \"    Testing the fitting\"\n",
    "    test_predict = lreg.predict(teFeatures)\n",
    "    confMat = confusion_matrix(teL,test_predict).astype(float)\n",
    "    normConfMat = confMat.copy()\n",
    "    normConfMat[0,:] /= confMat[0,:].sum()\n",
    "    normConfMat[1,:] /= confMat[1,:].sum()\n",
    "    return confMat, normConfMat, lreg.score(teFeatures, teL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordThreshold =  3 featureType =  1\n",
      "        Extracting Training Features\n",
      "        Extracting Testing Features\n",
      "    Fitting the regressor\n",
      "    Testing the fitting\n",
      "wordThreshold =  0 featureType =  1\n",
      "        Extracting Training Features\n",
      "        Extracting Testing Features\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 2704964)) while a minimum of 1 is required by the normalize function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-1c79ae35cd53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mncm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mncm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menglishReviews\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwordThreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatureType\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mncm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-83-02ffdb0516bf>\u001b[0m in \u001b[0;36mtrain_test_logistic_regression\u001b[1;34m(englishReviews, wordThreshold, featureType)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"wordThreshold = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordThreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"featureType = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureType\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mteD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mteL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseparateData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menglishReviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordThreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrFeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mteFeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextractFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mteD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"    Fitting the regressor\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-82-e5795826c5ca>\u001b[0m in \u001b[0;36mextractFeatures\u001b[1;34m(trData, teData, featureType)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"        Extracting Testing Features\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfeatureType\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mteFeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'English'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbagOfWordsShingles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mfeatureType\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mteFeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'English'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbagOfWordsShingles2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1354\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m         \u001b[1;31m# we set copy to False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1085\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     X = check_array(X, sparse_format, copy=copy, warn_on_dtype=True,\n\u001b[1;32m-> 1344\u001b[1;33m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    414\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[1;32m--> 416\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 2704964)) while a minimum of 1 is required by the normalize function."
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "cm,ncm,scr = train_test_logistic_regression(englishReviews,wordThreshold=3,featureType=1)\n",
    "results.append([cm,ncm,scr])\n",
    "\n",
    "cm,ncm,scr = train_test_logistic_regression(englishReviews,wordThreshold=0,featureType=1)\n",
    "results.append([cm,ncm,scr])\n",
    "\n",
    "cm,ncm,scr = train_test_logistic_regression(englishReviews,wordThreshold=3,featureType=2)\n",
    "results.append([cm,ncm,scr])\n",
    "\n",
    "cm,ncm,scr = train_test_logistic_regression(englishReviews,wordThreshold=0,featureType=2)\n",
    "results.append([cm,ncm,scr])\n",
    "\n",
    "for r in results:\n",
    "    print\n",
    "    print\n",
    "    print r[0]\n",
    "    print r[1]\n",
    "    print r[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[[ 2.  0.]\n",
      " [ 0.  1.]]\n",
      "[[ 1.  0.]\n",
      " [ 0.  1.]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    print\n",
    "    print\n",
    "    print r[0]\n",
    "    print r[1]\n",
    "    print r[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_LR(featMap, trueLabels, testSize=.33):\n",
    "    train, test, train_labels, test_labels = train_test_split(featMap,trueLabels,test_size=testSize,random_state=42)\n",
    "    train = MaxAbsScaler().fit_transform(train)\n",
    "    test = MaxAbsScaler().fit_transform(test)\n",
    "    \n",
    "    lreg = LogisticRegression(tol=0.001)\n",
    "    lreg.fit(train,train_labels)\n",
    "    test_predict = lreg.predict(test)\n",
    "    confMat = confusion_matrix(test_labels,test_predict).astype(float)\n",
    "    normConfMat = confMat.copy()\n",
    "    normConfMat[0,:] /= confMat[0,:].sum()\n",
    "    normConfMat[1,:] /= confMat[1,:].sum()\n",
    "    return confMat, normConfMat, lreg.score(test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets=['3WordThresholdedEnglishReviews', '3WordThresholdedEnglishReviews_k123', 'EnglishReviews', 'EnglishReviews_k123']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Loading and normalizing dataset: 3WordThresholdedEnglishReviews\n",
      "LR on 3WordThresholdedEnglishReviews\n",
      "    Normalized Confusion Matrix:\n",
      "     0.899443090067 0.100556909933\n",
      "     0.118478540221 0.881521459779\n",
      "    Accuracy:  0.889295277271\n",
      "\n",
      "\n",
      "\n",
      "Loading and normalizing dataset: 3WordThresholdedEnglishReviews_k123\n",
      "LR on 3WordThresholdedEnglishReviews_k123\n",
      "    Normalized Confusion Matrix:\n",
      "     0.960390060611 0.0396099393885\n",
      "     0.0506981924098 0.94930180759\n",
      "    Accuracy:  0.954111529357\n",
      "\n",
      "\n",
      "\n",
      "Loading and normalizing dataset: EnglishReviews\n",
      "LR on EnglishReviews\n",
      "    Normalized Confusion Matrix:\n",
      "     0.786778375431 0.213221624569\n",
      "     0.0475134582456 0.952486541754\n",
      "    Accuracy:  0.869883790613\n",
      "\n",
      "\n",
      "\n",
      "Loading and normalizing dataset: EnglishReviews_k123\n",
      "LR on EnglishReviews_k123\n",
      "    Normalized Confusion Matrix:\n",
      "     0.958877624371 0.041122375629\n",
      "     0.0634416152358 0.936558384764\n",
      "    Accuracy:  0.947684152648\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(len(datasets)):\n",
    "    min_max_scaler = MaxAbsScaler()\n",
    "    print \"\\n\\n\"\n",
    "    print \"Loading and normalizing dataset: \"+ datasets[i]\n",
    "    featureMatrix = sps.load_npz('./Datasets/'+datasets[i]+'_Features.npz')\n",
    "    #featureMatrix = min_max_scaler.fit_transform(featureMatrix)\n",
    "    labels = np.load('./Datasets/'+datasets[i]+'_Labels.npz')['arr_0']\n",
    "    print \"LR on \"+ datasets[i]\n",
    "    cm, ncm, sc = train_test_LR(featureMatrix, labels)\n",
    "    print \"    Normalized Confusion Matrix:\"\n",
    "    print \"    \", ncm[0,0], ncm[0,1]\n",
    "    print \"    \", ncm[1,0], ncm[1,1]\n",
    "    print \"    Accuracy: \", sc\n",
    "    results.append([datasets[i], cm.copy(), ncm.copy(), sc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
